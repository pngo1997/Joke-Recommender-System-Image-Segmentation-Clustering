{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0899912f-092f-41a0-96bc-14d517a6790a",
   "metadata": {},
   "source": [
    "#### Student Name: Mai Ngo\n",
    "#### Course Name and Number: DSC 478 Programming Machine Learning Applications\n",
    "#### Assignment 4\n",
    "#### Date: 6/4/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dec2f9-68d5-40e9-af0f-a626618055ee",
   "metadata": {},
   "source": [
    "### Part A. Recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe52f49-f693-41df-affa-f0243b92bf42",
   "metadata": {},
   "source": [
    "For this problem you will use a modified version of the item-based recommender algorithm from Ch. 14 of Machine Learning in Action and use it on joke ratings data based on Jester Online Joke Recommender System. The modified version of the code is provided in the module itemBasedRec.py. Most of the module will be used as is, but you will add some additional functionality.\n",
    "\n",
    "The data set contains two files. The file \"modified_jester_data.csv\" contains the ratings on 100 jokes by 1000 users (each row is a user profile). The ratings have been normalized to be between 1 and 21 (a 20-point scale), with 1 being the lowest rating. A zero indicated a missing rating. The file \"jokes.csv\" contains the joke ids mapped to the actual text of the jokes.\n",
    "\n",
    "Your tasks in this problem are the following (please also see comments for the function stubs in the provided module):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2548150-05a9-4fca-84e1-7766be9aeb62",
   "metadata": {},
   "source": [
    "#### a. Load in the joke ratings data and the joke text data into appropriate data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9068f50-b216-46d2-b596-3d46d8b22f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b2ffbe-d9e6-4150-a680-c79492721e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1000, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.18</td>\n",
       "      <td>19.79</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.17</td>\n",
       "      <td>2.02</td>\n",
       "      <td>6.24</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.08</td>\n",
       "      <td>10.71</td>\n",
       "      <td>17.36</td>\n",
       "      <td>15.37</td>\n",
       "      <td>8.62</td>\n",
       "      <td>1.34</td>\n",
       "      <td>10.27</td>\n",
       "      <td>5.66</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.22</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.71</td>\n",
       "      <td>18.86</td>\n",
       "      <td>10.81</td>\n",
       "      <td>8.86</td>\n",
       "      <td>14.06</td>\n",
       "      <td>11.34</td>\n",
       "      <td>6.68</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>19.16</td>\n",
       "      <td>8.18</td>\n",
       "      <td>17.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.50</td>\n",
       "      <td>15.61</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.61</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.60</td>\n",
       "      <td>18.04</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.56</td>\n",
       "      <td>16.73</td>\n",
       "      <td>...</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.58</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.73</td>\n",
       "      <td>12.55</td>\n",
       "      <td>14.11</td>\n",
       "      <td>17.55</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.83</td>\n",
       "      <td>7.46</td>\n",
       "      <td>11.44</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.91</td>\n",
       "      <td>6.68</td>\n",
       "      <td>2.31</td>\n",
       "      <td>10.13</td>\n",
       "      <td>4.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>...</td>\n",
       "      <td>7.46</td>\n",
       "      <td>4.11</td>\n",
       "      <td>10.32</td>\n",
       "      <td>8.04</td>\n",
       "      <td>8.82</td>\n",
       "      <td>7.65</td>\n",
       "      <td>11.05</td>\n",
       "      <td>1.92</td>\n",
       "      <td>5.95</td>\n",
       "      <td>7.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1.15</td>\n",
       "      <td>18.72</td>\n",
       "      <td>19.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17.84</td>\n",
       "      <td>14.16</td>\n",
       "      <td>20.17</td>\n",
       "      <td>4.79</td>\n",
       "      <td>2.84</td>\n",
       "      <td>9.30</td>\n",
       "      <td>20.27</td>\n",
       "      <td>12.41</td>\n",
       "      <td>5.81</td>\n",
       "      <td>6.58</td>\n",
       "      <td>...</td>\n",
       "      <td>18.23</td>\n",
       "      <td>9.88</td>\n",
       "      <td>10.90</td>\n",
       "      <td>5.32</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.65</td>\n",
       "      <td>13.14</td>\n",
       "      <td>10.95</td>\n",
       "      <td>12.31</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.21</td>\n",
       "      <td>7.46</td>\n",
       "      <td>1.58</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.26</td>\n",
       "      <td>10.71</td>\n",
       "      <td>5.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>3.14</td>\n",
       "      <td>9.40</td>\n",
       "      <td>...</td>\n",
       "      <td>15.37</td>\n",
       "      <td>10.71</td>\n",
       "      <td>15.17</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.01</td>\n",
       "      <td>16.15</td>\n",
       "      <td>16.15</td>\n",
       "      <td>14.01</td>\n",
       "      <td>17.41</td>\n",
       "      <td>16.15</td>\n",
       "      <td>19.93</td>\n",
       "      <td>13.52</td>\n",
       "      <td>14.01</td>\n",
       "      <td>19.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5      6      7      8      9   ...  \\\n",
       "0   3.18  19.79   1.34   2.84   3.48   2.50   1.15  15.17   2.02   6.24  ...   \n",
       "1  15.08  10.71  17.36  15.37   8.62   1.34  10.27   5.66  19.88  20.22  ...   \n",
       "2   0.00   0.00   0.00   0.00  20.03  20.27  20.03  20.27   0.00   0.00  ...   \n",
       "3   0.00  19.35   0.00   0.00  12.80  19.16   8.18  17.21   0.00  12.84  ...   \n",
       "4  19.50  15.61   6.83   5.61  12.36  12.60  18.04  15.61  10.56  16.73  ...   \n",
       "5   4.83   7.46  11.44   2.50   3.91   6.68   2.31  10.13   4.35   9.20  ...   \n",
       "6   0.00   0.00   0.00   0.00  19.59   1.15  18.72  19.79   0.00   0.00  ...   \n",
       "7  17.84  14.16  20.17   4.79   2.84   9.30  20.27  12.41   5.81   6.58  ...   \n",
       "8   7.21   7.46   1.58   4.11   2.26  10.71   5.71   2.07   3.14   9.40  ...   \n",
       "9  14.01  16.15  16.15  14.01  17.41  16.15  19.93  13.52  14.01  19.16  ...   \n",
       "\n",
       "      90     91     92     93     94     95     96     97     98     99  \n",
       "0  13.82   0.00   0.00   0.00   0.00   0.00   5.37   0.00   0.00   0.00  \n",
       "1  13.82   6.05  10.71  18.86  10.81   8.86  14.06  11.34   6.68  12.07  \n",
       "2   0.00   0.00   0.00  20.08   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "3   0.00   0.00   0.00  11.53   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "4  16.19  16.58  15.27  16.19  16.73  12.55  14.11  17.55  12.80  12.60  \n",
       "5   7.46   4.11  10.32   8.04   8.82   7.65  11.05   1.92   5.95   7.55  \n",
       "6   0.00   0.00   0.00   0.00   0.00  13.33   0.00   0.00   0.00   0.00  \n",
       "7  18.23   9.88  10.90   5.32   7.84   7.65  13.14  10.95  12.31  11.00  \n",
       "8  15.37  10.71  15.17  10.71  10.71  10.71  10.71  10.71   7.60   6.05  \n",
       "9   0.00  15.47   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokeRating = pd.read_csv('modified_jester_data.csv', header=None)\n",
    "print (f\"Data shape: {jokeRating.shape}\")\n",
    "jokeRating.head(10)\n",
    "#Read data as Pandas dataframe and get number of rows and columns, print first 10 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faba2e40-bb49-4f7c-8e4d-66475023173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (100, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['A man visits the doctor. The doctor says \"I have bad news for you.You have cancer and Alzheimer\\'s disease\". The man replies \"Well thank God I don\\'t have cancer!\"',\n",
       "       'This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\" \"They told me that you were a pedophile.\" He replied \"That\\'s an awfully big word for a ten year old.\"',\n",
       "       \"Q. What's 200 feet long and has 4 teeth? A. The front row at a Willie Nelson Concert.\",\n",
       "       \"Q. What's the difference between a man and a toilet? A. A toilet doesn't follow you around after you use it.\",\n",
       "       \"Q. What's O. J. Simpson's Internet address? A.\\tSlash slash backslash slash slash escape.\",\n",
       "       \"Bill & Hillary are on a trip back to Arkansas. They're almost out of gas so Bill pulls into a service station on the outskirts of town. The attendant runs out of the station to serve them when Hillary realizes it's an old boyfriend from high school. She and the attendant chat as he gases up their car and cleans the windows. Then they all say good-bye. As Bill pulls the car onto the road he turns to Hillary and says 'Now aren't you glad you married me and not him ? You could've been the wife of a grease monkey !' To which Hillary replied 'No Bill. If I would have married him you'd be pumping gas and he would be the President !'\",\n",
       "       \"How many feminists does it take to screw in a light bulb?That's not funny.\",\n",
       "       'Q. Did you hear about the dyslexic devil worshipper? A. He sold his soul to Santa.',\n",
       "       'A country guy goes into a city bar that has a dress code and the maitred\\' demands he wear a tie. Discouraged the guy goes to his car to sulk when inspiration strikes: He\\'s got jumper cables in the trunk! So he wraps them around his neck sort of like a string tie (a bulky string tie to be sure) and returns to the bar. The maitre d\\' is reluctant but says to the guy \"Okay you\\'re a pretty resourceful fellow you can come in... but just don\\'t start anything\"!',\n",
       "       'Two cannibals are eating a clown one turns to other and says: \"Does this taste funny to you?'],\n",
       "      dtype='<U1198')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokeText = np.genfromtxt(\"jokes.csv\", delimiter=',', dtype=str)\n",
    "print (f\"Data shape: {np.shape(jokeText)}\")\n",
    "jokeText = jokeText[:,1]\n",
    "jokeText[:10]\n",
    "#Read data as numpy array and get number of rows and columns, print first 10 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed61c1fe-cb17-4d4d-8d58-53a7a95dcaa9",
   "metadata": {},
   "source": [
    "#### b. Complete the definition for the function \"test\". This function iterates over all users and for each performs evaluation (by calling the provided \"cross_validate_user\" function), and returns the error information necessary to compute Mean Absolute Error (MAE). Use this function to perform evaluation (with 20% test-ratio for each user) comparing MAE results using standard item-based collaborative filtering (based on the rating prediction function \"standEst\") with results using the SVD-based version of the rating item-based CF (using \"svdEst\" as the prediction engine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145cbd51-0d79-4379-babb-ef0dfc0d1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidSim(userA, userB):\n",
    "    '''Euclidean similarity between two user ratings.'''\n",
    "    return 1.0/(1.0 + la.norm(userA - userB))\n",
    "\n",
    "def pearsonSim(userA, userB):\n",
    "    '''Pearson similarity between two user ratings.'''\n",
    "    if len(userA) < 3 : return 1.0\n",
    "    return 0.5+0.5*np.corrcoef(userA, userB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosineSim(userA, userB):\n",
    "    '''Cosine similarity between two user ratings.'''\n",
    "    dotProduct = float(np.dot(userA.T, userB))\n",
    "    normProduct = np.linalg.norm(userA)*np.linalg.norm(userB)\n",
    "    return 0.5+0.5*(dotProduct/normProduct)\n",
    "    #Rescale to the range [0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b911880-9e2e-4920-9b18-c690b898288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standEst(dataMatrix, user, simMeas, item):\n",
    "    '''Returns estimated rating (weighted similarity) of the 'user' for the 'item' using standard item-based collaborative filtering.\n",
    "    user: Index of the iterated user. item: index of the iterated joke.'''\n",
    "    \n",
    "    n = np.shape(dataMatrix)[1]\n",
    "    #Get total number of jokes/items.\n",
    "    simTotal = 0.0\n",
    "    #Total similarity.\n",
    "    ratSimTotal = 0.0\n",
    "    #Total weighted similarity.\n",
    "    \n",
    "    for j in range(n):\n",
    "        userRating = dataMatrix[user,j]\n",
    "        #Get original rating of iterated 'user' for item j-th.\n",
    "        if userRating == 0: continue\n",
    "        #If user gives rating=0, we skip.\n",
    "        \n",
    "        overLap = np.nonzero(np.logical_and(dataMatrix[:,item]>0, dataMatrix[:,j]>0))[0]\n",
    "        #Get index the user who have rated both the current item (item) and item (j-th).\n",
    "        if len(overLap) == 0: similarity = 0\n",
    "        #If ratings are the same, similarity is 0.\n",
    "        else: similarity = simMeas(dataMatrix[overLap,item], dataMatrix[overLap,j])\n",
    "        #Otherwise, calculate similarity between the ratings of users who have rated both the current item (item) and item (j-th).\n",
    "        \n",
    "        simTotal += similarity\n",
    "        #Accumulate similarity.\n",
    "        ratSimTotal += similarity * userRating\n",
    "        #Accumulate weighted similarity.\n",
    "        \n",
    "    if simTotal == 0: return 0\n",
    "    #If there is no overlap in rating, returns 0.\n",
    "    else: return ratSimTotal/simTotal\n",
    "    #Returns weighted similarity of userRating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9690971b-50c3-48eb-a699-30eee985fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svdEst(dataMatrix, user, simMeas, item):\n",
    "    '''Returns estimated rating (weighted similarity) of the 'user' for the 'item' using SVD-based version of the rating item-based collaborative filtering.\n",
    "    user: Index of the iterated user. item: index of the iterated joke.'''\n",
    "    \n",
    "    n = np.shape(dataMatrix)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data = np.mat(dataMatrix)\n",
    "    #Convert to data matrix for Singular Value Decomposition (SVD).\n",
    "    \n",
    "    U,Sigma,VT = la.svd(data)\n",
    "    #U, Sigma,and VT form the SVD factorization of dataMatrix.\n",
    "    \n",
    "    Sig4 = np.mat(np.eye(4)*Sigma[:4])\n",
    "    #Using the first 4 singular values from itemBasedRec.py file.\n",
    "    #Arrange Sig4 into a diagonal matrix.\n",
    "    xformedItems = data.T * U[:,:4] * Sig4.I\n",
    "    #Transpose data into Item-User matrix.\n",
    "    #Multiply transposed data matrix with the first 4 columns of the U matrix, then multiply with the inverse of Sig4 matrix.\n",
    "    #xformedItems represents the transformed items based on the SVD decomposition.\n",
    "    \n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0 or j==item: continue\n",
    "        #If user gives rating=0 or current iterated item, we skip.\n",
    "        similarity = simMeas(xformedItems[item,:], xformedItems[j,:])\n",
    "        #Otherwise, calculate similarity between the ratings of users who have rated both the current item (item) and item (j-th).\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal\n",
    "    #Returns weighted similarity of userRating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca916c2-3433-4758-addf-8ef60a0f06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validateUser(dataMatrix, user, testRatio, estMethod=standEst, simMeas=pearsonSim):\n",
    "    '''Perform cross-validation for given user.\n",
    "    Return sum of absolute errors, count of test ratings to compute Mean Absolute Error (MAE).'''\n",
    "    \n",
    "    itemNumbers = np.shape(dataMatrix)[1]   \n",
    "    #Get total number of jokes.\n",
    "    avail_userRating = np.array([i for i in range(itemNumbers) if dataMatrix[user,i]>0])\n",
    "    #Get indicies of ratings are available (ratings>0) by the 'user'. \n",
    "    testSize = int(testRatio * len(avail_userRating))\n",
    "    \n",
    "    #Get total number of ratings withheld for testing based on available ratings per user.\n",
    "    testIndices = np.random.randint(0, len(avail_userRating), testSize)\n",
    "    #Generate random indices from the available ratings to select the test items for testing.\n",
    "    testItems = np.array(avail_userRating)[testIndices]\n",
    "    #Get indicies of test items.\n",
    "    \n",
    "    orig_userRating = np.copy(dataMatrix[user])\n",
    "    #Make a copy of original rating of the 'user', including 0 value(s).\n",
    "    #orig_userRating has shape (1, 100)\n",
    "    dataMatrix[user, testItems] = 0 \n",
    "    #Set test items values to 0, so they are not used in rating estimation.\n",
    "    error_userRating = 0.0\n",
    "    #Initialize counter for absolute error of the 'user' ratings.\n",
    "    count_userRating = len(testItems)\n",
    "    #Get the count of test items.\n",
    "\n",
    "    for item in testItems:\n",
    "        estimatedScore = estMethod(dataMatrix, user, simMeas, item)\n",
    "        #Estimate rating on test item. \n",
    "        error_userRating += abs(estimatedScore - orig_userRating[0, item])\n",
    "        #Accumulate absolute error by comparing estimated vs. original rating. \n",
    "\n",
    "    for item in testItems:\n",
    "        dataMatrix[user, item] = orig_userRating[0, item]\n",
    "        #Restore ratings of the test items to the user original ratings profile.\n",
    "        \n",
    "    return error_userRating, count_userRating\n",
    "    #Return sum of absolute errors and the count of test cases for this user. \n",
    "    #Will use this calculate MAE for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c20cc709-525b-4152-92c9-9620cc088529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataMatrix, testRatio, estMethod):\n",
    "    '''Calculate Mean Absolute Error.'''\n",
    "    \n",
    "    totalErrors = 0\n",
    "    totalCounts = 0\n",
    "    \n",
    "    for user in range(len(dataMatrix)):\n",
    "        error_userRating, count_userRating = cross_validateUser(dataMatrix, user, testRatio, estMethod=standEst, simMeas=pearsonSim)\n",
    "        totalErrors += error_userRating\n",
    "        totalCounts += count_userRating\n",
    "    \n",
    "    MAE = totalErrors/totalCounts\n",
    "    print(f\"Mean Absolute Error for {estMethod.__name__}: {MAE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb47e298-b607-429d-954f-8468ef596815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for standEst: 3.6866361996922237\n",
      "Wall time: 8min 18s\n"
     ]
    }
   ],
   "source": [
    "jokeRating_matrix = np.mat(jokeRating)\n",
    "%time test(jokeRating_matrix, 0.2, standEst)\n",
    "#Using standard estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0551da3-7d95-428e-963e-45c510dc7293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for svdEst: 3.6893545892480515\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%time test(jokeRating_matrix, 0.2, svdEst)\n",
    "#Using SVD estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3049d28-1b59-4b81-8201-eefe5f487afa",
   "metadata": {},
   "source": [
    "The MAE value for standard estimate is 3.6866; and the MAE value for SVD estimate is 3.7159. SVD estimate yields higher MAE than Standard estimate but the indifference gap is not significant. This slight difference shows that both methods perform similarly in rating estimations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4ce83-fda3-4567-af3d-3dad69311743",
   "metadata": {},
   "source": [
    "#### c. Write a new function \"print_most_similar_jokes\" which takes the joke ratings data, a query joke id, a parameter k for the number of nearest neighbors, and a similarity metric function, and prints the text of the query joke as well as the texts of the top k most similar jokes based on user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257efc22-6db7-4a85-8cb5-7e519edb652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_jokes(dataMatrix, jokeText, jokeID, k, metric=pearsonSim):\n",
    "    '''Prints the text of the query joke as well as the texts of the top k most similar jokes based on user ratings.'''\n",
    "    \n",
    "    queryJoke_ratings = dataMatrix[:, jokeID]\n",
    "    #Get the ratings of the query joke.\n",
    "    \n",
    "    similarities = []\n",
    "    #Compute the similarity scores between the query joke and all other jokes.\n",
    "    for ratingIndex, ratings in enumerate(dataMatrix.T):\n",
    "        #Iterate over each joke (transpose rating data to Iterm-User matrix).\n",
    "        if  ratingIndex!= jokeID:\n",
    "        #Exclue query joke.\n",
    "            similarity = metric(queryJoke_ratings, ratings)\n",
    "            similarities.append((ratingIndex, similarity))\n",
    "            #Calculate and append similarity.\n",
    "            \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    #Sort similarities in descending order.\n",
    "    \n",
    "    print(f\"Query Joke: {jokeText[jokeID]}\")\n",
    "    print()\n",
    "    #Print query joke.\n",
    "    \n",
    "    print(f\"Top {k} Most Similar Jokes:\")\n",
    "    for i in range(k):\n",
    "        jokeIndex, similarity = similarities[i]\n",
    "        sim_jokeText = jokeText[jokeIndex]\n",
    "        print(f\"Joke ID: {jokeIndex}\")\n",
    "        print(f\"Similarity: {similarity}\")\n",
    "        print(sim_jokeText)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07a5255c-e491-4a67-b203-9e1e7b874638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Joke: This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\" \"They told me that you were a pedophile.\" He replied \"That's an awfully big word for a ten year old.\"\n",
      "\n",
      "Top 3 Most Similar Jokes:\n",
      "Joke ID: 0\n",
      "Similarity: 0.7858504010765623\n",
      "A man visits the doctor. The doctor says \"I have bad news for you.You have cancer and Alzheimer's disease\". The man replies \"Well thank God I don't have cancer!\"\n",
      "\n",
      "Joke ID: 2\n",
      "Similarity: 0.725831124183739\n",
      "Q. What's 200 feet long and has 4 teeth? A. The front row at a Willie Nelson Concert.\n",
      "\n",
      "Joke ID: 36\n",
      "Similarity: 0.722828904812725\n",
      "A Jewish young man was seeing a psychiatrist for an eating and sleeping disorder. \"I am so obsessed with my mother... As soon as I go to sleep I start dreaming and everyone in my dream turns into my mother. I wake up in such a state all I can do is go downstairs and eat a piece of toast.\"The psychiatrist replies:\"What just one piece of toast for a big boy like you?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_most_similar_jokes(jokeRating_matrix, jokeText, 1, 3, pearsonSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a94545-250d-432a-b9c0-9ac75822bb09",
   "metadata": {},
   "source": [
    "### Part B. PCA for Reduced Dimensionality in Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e70489-def3-47ac-a3e8-134f1dbdf851",
   "metadata": {},
   "source": [
    "For this problem you will use an image segmentation data set for clustering. You will experiment with using PCA as an approach to reduce dimensionality and noise in the data. You will compare the results of clustering the data with and without PCA using the provided image class assignments as the ground truth. The data set is divided into three files. The file \"segmentation_data.txt\" contains data about images with each line corresponding to one image. Each image is represented by 19 features (these are the columns in the data and correspond to the feature names in the file \"segmentation_names.txt\". The file \"segmentation_classes.txt\" contains the class labels (the type of image) and a numeric class label for each of the corresponding images in the data file. After clustering the image data, you will use the class labels to measure completeness and homogeneity of the generated clusters. The data set used in this problem is based on the Image Segmentation data set at the UCI Machine Learning Repository.\n",
    "\n",
    "Your tasks in this problem are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d4042-1e66-4489-9931-942ab681a61a",
   "metadata": {},
   "source": [
    "#### a. Load in the image data matrix (with rows as images and columns as features). Also load in the numeric class labels from the segmentation class file. Using your favorite method (e.g., sklearn's min-max scaler), perform min-max normalization on the data matrix so that each feature is scaled to [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4351b5c-62c8-4693-8e02-cebe120091b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2100, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>1.186342</td>\n",
       "      <td>12.925926</td>\n",
       "      <td>10.888889</td>\n",
       "      <td>9.222222</td>\n",
       "      <td>18.666668</td>\n",
       "      <td>-6.111111</td>\n",
       "      <td>-11.111111</td>\n",
       "      <td>17.222221</td>\n",
       "      <td>18.666668</td>\n",
       "      <td>0.508139</td>\n",
       "      <td>1.910864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.720082</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.750309</td>\n",
       "      <td>13.740741</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>10.333334</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>-6.222222</td>\n",
       "      <td>-10.222222</td>\n",
       "      <td>16.444445</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>0.463329</td>\n",
       "      <td>1.941465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>2.195113</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.520234</td>\n",
       "      <td>12.259259</td>\n",
       "      <td>10.333334</td>\n",
       "      <td>9.333334</td>\n",
       "      <td>17.111110</td>\n",
       "      <td>-5.777778</td>\n",
       "      <td>-8.777778</td>\n",
       "      <td>14.555555</td>\n",
       "      <td>17.111110</td>\n",
       "      <td>0.480149</td>\n",
       "      <td>1.987902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>1.254621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>12.703704</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>18.111110</td>\n",
       "      <td>-5.111111</td>\n",
       "      <td>-11.111111</td>\n",
       "      <td>16.222221</td>\n",
       "      <td>18.111110</td>\n",
       "      <td>0.500966</td>\n",
       "      <td>1.875362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.691215</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1.005540</td>\n",
       "      <td>15.592592</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>11.777778</td>\n",
       "      <td>21.111110</td>\n",
       "      <td>-5.111111</td>\n",
       "      <td>-11.444445</td>\n",
       "      <td>16.555555</td>\n",
       "      <td>21.111110</td>\n",
       "      <td>0.442661</td>\n",
       "      <td>1.863654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>157.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.055556</td>\n",
       "      <td>0.646930</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>0.620633</td>\n",
       "      <td>12.111111</td>\n",
       "      <td>10.222222</td>\n",
       "      <td>8.111112</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>-5.666666</td>\n",
       "      <td>-12.000000</td>\n",
       "      <td>17.666666</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.549180</td>\n",
       "      <td>1.877146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.944445</td>\n",
       "      <td>1.083547</td>\n",
       "      <td>2.333334</td>\n",
       "      <td>1.632993</td>\n",
       "      <td>14.629630</td>\n",
       "      <td>13.222222</td>\n",
       "      <td>11.444445</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>-4.222222</td>\n",
       "      <td>-9.555555</td>\n",
       "      <td>13.777778</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>0.408965</td>\n",
       "      <td>1.860191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.611111</td>\n",
       "      <td>0.646930</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>1.722401</td>\n",
       "      <td>15.296296</td>\n",
       "      <td>14.777778</td>\n",
       "      <td>12.888889</td>\n",
       "      <td>18.222221</td>\n",
       "      <td>-1.555556</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>8.777778</td>\n",
       "      <td>18.222221</td>\n",
       "      <td>0.312227</td>\n",
       "      <td>1.783512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>2.146487</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>1.327766</td>\n",
       "      <td>14.481482</td>\n",
       "      <td>12.555555</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>19.555555</td>\n",
       "      <td>-5.777778</td>\n",
       "      <td>-9.444445</td>\n",
       "      <td>15.222222</td>\n",
       "      <td>19.555555</td>\n",
       "      <td>0.422174</td>\n",
       "      <td>1.950405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>1.985130</td>\n",
       "      <td>2.444445</td>\n",
       "      <td>1.614747</td>\n",
       "      <td>13.703704</td>\n",
       "      <td>11.222222</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>18.777779</td>\n",
       "      <td>-7.444445</td>\n",
       "      <td>-7.777778</td>\n",
       "      <td>15.222222</td>\n",
       "      <td>18.777779</td>\n",
       "      <td>0.439852</td>\n",
       "      <td>2.099905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1   2         3    4         5         6         7         8   \\\n",
       "0  110.0  189.0   9  0.000000  0.0  1.000000  0.666667  1.222222  1.186342   \n",
       "1   86.0  187.0   9  0.000000  0.0  1.111111  0.720082  1.444444  0.750309   \n",
       "2  225.0  244.0   9  0.000000  0.0  3.388889  2.195113  3.000000  1.520234   \n",
       "3   47.0  232.0   9  0.000000  0.0  1.277778  1.254621  1.000000  0.894427   \n",
       "4   97.0  186.0   9  0.000000  0.0  1.166667  0.691215  1.166667  1.005540   \n",
       "5  157.0  221.0   9  0.000000  0.0  1.055556  0.646930  1.222222  0.620633   \n",
       "6   62.0  224.0   9  0.000000  0.0  0.944445  1.083547  2.333334  1.632993   \n",
       "7   27.0  248.0   9  0.111111  0.0  1.611111  0.646930  3.166667  1.722401   \n",
       "8   44.0  233.0   9  0.000000  0.0  2.222222  2.146487  2.111111  1.327766   \n",
       "9   17.0  229.0   9  0.000000  0.0  2.111111  1.985130  2.444445  1.614747   \n",
       "\n",
       "          9          10         11         12        13         14         15  \\\n",
       "0  12.925926  10.888889   9.222222  18.666668 -6.111111 -11.111111  17.222221   \n",
       "1  13.740741  11.666667  10.333334  19.222221 -6.222222 -10.222222  16.444445   \n",
       "2  12.259259  10.333334   9.333334  17.111110 -5.777778  -8.777778  14.555555   \n",
       "3  12.703704  11.000000   9.000000  18.111110 -5.111111 -11.111111  16.222221   \n",
       "4  15.592592  13.888889  11.777778  21.111110 -5.111111 -11.444445  16.555555   \n",
       "5  12.111111  10.222222   8.111112  18.000000 -5.666666 -12.000000  17.666666   \n",
       "6  14.629630  13.222222  11.444445  19.222221 -4.222222  -9.555555  13.777778   \n",
       "7  15.296296  14.777778  12.888889  18.222221 -1.555556  -7.222222   8.777778   \n",
       "8  14.481482  12.555555  11.333333  19.555555 -5.777778  -9.444445  15.222222   \n",
       "9  13.703704  11.222222  11.111111  18.777779 -7.444445  -7.777778  15.222222   \n",
       "\n",
       "          16        17        18  \n",
       "0  18.666668  0.508139  1.910864  \n",
       "1  19.222221  0.463329  1.941465  \n",
       "2  17.111110  0.480149  1.987902  \n",
       "3  18.111110  0.500966  1.875362  \n",
       "4  21.111110  0.442661  1.863654  \n",
       "5  18.000000  0.549180  1.877146  \n",
       "6  19.222221  0.408965  1.860191  \n",
       "7  18.222221  0.312227  1.783512  \n",
       "8  19.555555  0.422174  1.950405  \n",
       "9  18.777779  0.439852  2.099905  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segData = pd.read_csv(\"segmentation_data.txt\", header = None)\n",
    "print (f\"Data shape: {segData.shape}\")\n",
    "segData.head(10)\n",
    "#Read data as Pandas dataframe and get number of rows and columns, print first 10 observations.\n",
    "#2100 images, 19 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "582fe77f-2772-4793-98dc-5f2af2ba9917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2100, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Value\n",
       "0  GRASS      0\n",
       "1  GRASS      0\n",
       "2  GRASS      0\n",
       "3  GRASS      0\n",
       "4  GRASS      0\n",
       "5  GRASS      0\n",
       "6  GRASS      0\n",
       "7  GRASS      0\n",
       "8  GRASS      0\n",
       "9  GRASS      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segClass = pd.read_csv(\"segmentation_classes.txt\", header = None, sep ='\\t', names = ['Name', 'Value'])\n",
    "print (f\"Data shape: {segClass.shape}\")\n",
    "segClass.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8dbf1ed-88d0-425c-97f2-1df7e37b176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (2100, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REGION-CENTROID-COL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REGION-CENTROID-ROW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REGION-PIXEL-COUNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHORT-LINE-DENSITY-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SHORT-LINE-DENSITY-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VEDGE-MEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VEDGE-SD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HEDGE-MEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HEDGE-SD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INTENSITY-MEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "0   REGION-CENTROID-COL\n",
       "1   REGION-CENTROID-ROW\n",
       "2    REGION-PIXEL-COUNT\n",
       "3  SHORT-LINE-DENSITY-5\n",
       "4  SHORT-LINE-DENSITY-2\n",
       "5            VEDGE-MEAN\n",
       "6              VEDGE-SD\n",
       "7            HEDGE-MEAN\n",
       "8              HEDGE-SD\n",
       "9        INTENSITY-MEAN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segName = pd.read_csv(\"segmentation_names.txt\", header = None, sep ='\\t')\n",
    "print (f\"Data shape: {segClass.shape}\")\n",
    "segName.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3419ae6c-b536-4250-8ddb-337ac79036da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43083004, 0.74166667, 0.        , ..., 0.12371135, 0.50813884,\n",
       "        0.83184923],\n",
       "       [0.33596838, 0.73333333, 0.        , ..., 0.12739322, 0.46332908,\n",
       "        0.83698646],\n",
       "       [0.88537549, 0.97083333, 0.        , ..., 0.11340205, 0.48014903,\n",
       "        0.84478233],\n",
       "       ...,\n",
       "       [0.50197628, 0.625     , 0.        , ..., 0.07216495, 0.5409177 ,\n",
       "        0.17591546],\n",
       "       [0.58893281, 0.6125    , 0.        , ..., 0.08100147, 0.50308645,\n",
       "        0.18478933],\n",
       "       [0.48616601, 0.62916667, 0.        , ..., 0.09646539, 0.4799313 ,\n",
       "        0.17037463]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(segData)\n",
    "segData_norm = scaler.transform(segData)\n",
    "#Apply min-max normalization on the segData matrix so that each feature is scaled to [0,1] range\n",
    "segData_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea08c23-574d-4bde-b84d-c796b3b42c76",
   "metadata": {},
   "source": [
    "#### b. Next, Perform Kmeans clustering (for this problem, use the Kmeans implementation in scikit-learn) on the image data (since there are a total 7 pre-assigned image classes, you should use K = 7 in your clustering). Use Euclidean distance as your distance measure for the clustering. Print the cluster centroids (use some formatting so that they are visually understandable). Compare your 7 clusters to the 7 pre-assigned classes by computing the Completeness and Homogeneity values of the generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "261cd3ac-6752-415d-8609-b08a01ae3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import completeness_score, homogeneity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46bea7bf-9678-4f67-a192-712370262490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 7\n",
    "kMeans = KMeans(n_clusters=k)\n",
    "kMeans.fit(segData_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52c34fda-0fe7-41c7-a3da-afe56d19e339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centroids:\n",
      "[0.25121165 0.39336558 0.         0.07539682 0.01934524 0.07800901\n",
      " 0.00443644 0.06225598 0.0053475  0.14728647 0.13735819 0.18421041\n",
      " 0.11752357 0.71839464 0.34356793 0.35466366 0.18452162 0.41410698\n",
      " 0.20277198]\n",
      "\n",
      "[5.35098814e-01 1.50166667e-01 0.00000000e+00 2.77777769e-02\n",
      " 1.66666667e-03 3.02281387e-02 5.42887957e-04 2.67660451e-02\n",
      " 5.86661900e-04 8.23246433e-01 7.79716377e-01 8.94170356e-01\n",
      " 7.88760696e-01 2.70665440e-01 6.66372551e-01 2.89386481e-01\n",
      " 8.94170356e-01 2.11804171e-01 1.25065773e-01]\n",
      "\n",
      "[0.76906251 0.42593042 0.         0.01402373 0.02265372 0.03970246\n",
      " 0.00298261 0.02311604 0.00209423 0.04038497 0.03442642 0.05738498\n",
      " 0.02805924 0.77991717 0.2227946  0.48688616 0.05836205 0.53915212\n",
      " 0.24498804]\n",
      "\n",
      "[0.51399369 0.80893659 0.         0.07744108 0.00505051 0.05447376\n",
      " 0.00140719 0.04633498 0.00140097 0.10878994 0.09140296 0.09241408\n",
      " 0.14267644 0.67916102 0.07900179 0.82128688 0.1349008  0.41449132\n",
      " 0.89233263]\n",
      "\n",
      "[0.30250553 0.53086158 0.         0.05225989 0.04661017 0.10081685\n",
      " 0.00942022 0.08397199 0.01104328 0.40060809 0.37034723 0.47246075\n",
      " 0.35303578 0.49714616 0.57088236 0.2130544  0.47246075 0.30226303\n",
      " 0.16387917]\n",
      "\n",
      "[0.2541502  0.45938218 0.         0.02643678 0.0137931  0.03679035\n",
      " 0.00203123 0.02660955 0.00165141 0.02568712 0.01750964 0.04199888\n",
      " 0.01624694 0.76928076 0.21602029 0.50863052 0.04291047 0.80315741\n",
      " 0.18040715]\n",
      "\n",
      "[0.74827373 0.53204066 0.         0.03915663 0.0376506  0.11353017\n",
      " 0.01892238 0.10731122 0.017627   0.29857308 0.27752095 0.35008073\n",
      " 0.2638371  0.59330045 0.44924168 0.31145266 0.3501628  0.30304693\n",
      " 0.1643593 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Cluster Centroids:\")\n",
    "for centroid in kMeans.cluster_centers_:\n",
    "    print(centroid)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "948dee04-c880-452d-9125-5564c0c8ec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centroids:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster 1</th>\n",
       "      <th>Cluster 2</th>\n",
       "      <th>Cluster 3</th>\n",
       "      <th>Cluster 4</th>\n",
       "      <th>Cluster 5</th>\n",
       "      <th>Cluster 6</th>\n",
       "      <th>Cluster 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(REGION-CENTROID-COL,)</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(REGION-CENTROID-ROW,)</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(REGION-PIXEL-COUNT,)</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(SHORT-LINE-DENSITY-5,)</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(SHORT-LINE-DENSITY-2,)</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(VEDGE-MEAN,)</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(VEDGE-SD,)</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(HEDGE-MEAN,)</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(HEDGE-SD,)</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(INTENSITY-MEAN,)</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(RAWRED-MEAN,)</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(RAWBLUE-MEAN,)</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(RAWGREEN-MEAN,)</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(EXRED-MEAN,)</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(EXBLUE-MEAN,)</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(EXGREEN-MEAN,)</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(VALUE-MEAN,)</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(SATURATION-MEAN,)</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(HUE-MEAN,)</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Cluster 1  Cluster 2  Cluster 3  Cluster 4  \\\n",
       "(REGION-CENTROID-COL,)        0.25       0.54       0.77       0.51   \n",
       "(REGION-CENTROID-ROW,)        0.39       0.15       0.43       0.81   \n",
       "(REGION-PIXEL-COUNT,)         0.00       0.00       0.00       0.00   \n",
       "(SHORT-LINE-DENSITY-5,)       0.08       0.03       0.01       0.08   \n",
       "(SHORT-LINE-DENSITY-2,)       0.02       0.00       0.02       0.01   \n",
       "(VEDGE-MEAN,)                 0.08       0.03       0.04       0.05   \n",
       "(VEDGE-SD,)                   0.00       0.00       0.00       0.00   \n",
       "(HEDGE-MEAN,)                 0.06       0.03       0.02       0.05   \n",
       "(HEDGE-SD,)                   0.01       0.00       0.00       0.00   \n",
       "(INTENSITY-MEAN,)             0.15       0.82       0.04       0.11   \n",
       "(RAWRED-MEAN,)                0.14       0.78       0.03       0.09   \n",
       "(RAWBLUE-MEAN,)               0.18       0.89       0.06       0.09   \n",
       "(RAWGREEN-MEAN,)              0.12       0.79       0.03       0.14   \n",
       "(EXRED-MEAN,)                 0.72       0.27       0.78       0.68   \n",
       "(EXBLUE-MEAN,)                0.34       0.67       0.22       0.08   \n",
       "(EXGREEN-MEAN,)               0.35       0.29       0.49       0.82   \n",
       "(VALUE-MEAN,)                 0.18       0.89       0.06       0.13   \n",
       "(SATURATION-MEAN,)            0.41       0.21       0.54       0.41   \n",
       "(HUE-MEAN,)                   0.20       0.13       0.24       0.89   \n",
       "\n",
       "                         Cluster 5  Cluster 6  Cluster 7  \n",
       "(REGION-CENTROID-COL,)        0.30       0.25       0.75  \n",
       "(REGION-CENTROID-ROW,)        0.53       0.46       0.53  \n",
       "(REGION-PIXEL-COUNT,)         0.00       0.00       0.00  \n",
       "(SHORT-LINE-DENSITY-5,)       0.05       0.03       0.04  \n",
       "(SHORT-LINE-DENSITY-2,)       0.05       0.01       0.04  \n",
       "(VEDGE-MEAN,)                 0.10       0.04       0.11  \n",
       "(VEDGE-SD,)                   0.01       0.00       0.02  \n",
       "(HEDGE-MEAN,)                 0.08       0.03       0.11  \n",
       "(HEDGE-SD,)                   0.01       0.00       0.02  \n",
       "(INTENSITY-MEAN,)             0.40       0.03       0.30  \n",
       "(RAWRED-MEAN,)                0.37       0.02       0.28  \n",
       "(RAWBLUE-MEAN,)               0.47       0.04       0.35  \n",
       "(RAWGREEN-MEAN,)              0.35       0.02       0.26  \n",
       "(EXRED-MEAN,)                 0.50       0.77       0.59  \n",
       "(EXBLUE-MEAN,)                0.57       0.22       0.45  \n",
       "(EXGREEN-MEAN,)               0.21       0.51       0.31  \n",
       "(VALUE-MEAN,)                 0.47       0.04       0.35  \n",
       "(SATURATION-MEAN,)            0.30       0.80       0.30  \n",
       "(HUE-MEAN,)                   0.16       0.18       0.16  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing cluster centroids\n",
    "pd.options.display.float_format='{:,.2f}'.format\n",
    "centroids = pd.DataFrame(kMeans.cluster_centers_, columns=segName)\n",
    "centroidsTranspose = centroids.T\n",
    "centroidsTranspose.columns = [\"Cluster {}\".format(i+1) for i in range(len(centroidsTranspose.columns))]\n",
    "print(\"Cluster Centroids:\")\n",
    "centroidsTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "096ab0f1-40ff-4e68-b677-3d922f752c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness with full normalized dataset: 0.6116744999910891\n",
      "Homogeneity with full normalized dataset: 0.6099656393147241\n"
     ]
    }
   ],
   "source": [
    "completeness = completeness_score(segClass['Value'], kMeans.labels_)\n",
    "homogeneity = homogeneity_score(segClass['Value'], kMeans.labels_)\n",
    "\n",
    "# Print the completeness and homogeneity scores\n",
    "print(f\"Completeness with full normalized dataset: {completeness}\")\n",
    "print(f\"Homogeneity with full normalized dataset: {homogeneity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ce96a-dbd0-447c-a09e-bde0134927ca",
   "metadata": {},
   "source": [
    "Completeness score measures how much all data points belong to the same class are assigned to the same cluster. With completeness score = 0.6132 incdicates that 61.32% of data poinits in each class are correctly assigned to the same cluster. \\\n",
    "Homogeneity score measures how much each cluster contains only data points of a certain class label. Homogeneity score = 0.61 indicates that 61% of the data points within each cluster belong to the same class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c57ab7-a20a-4e4e-9e42-eda19149f6d0",
   "metadata": {},
   "source": [
    "#### c. Perform PCA on the normalized image data matrix. You may use the linear algebra package in Numpy or the Decomposition module in scikit-learn (the latter is much more efficient). Analyze the principal components to determine the number, r, of PCs needed to capture at least 95% of variance in the data. Then use these r components as features to transform the data into a reduced dimension space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe994a76-ef07-41df-9735-c770ec86e2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.89082181e-01,  5.32951033e-01,  2.46098334e-01, ...,\n",
       "        -3.11214082e-09,  1.10281139e-08,  9.50161528e-18],\n",
       "       [-6.66919524e-01,  5.10674647e-01,  3.37971716e-01, ...,\n",
       "         2.10646100e-09, -9.82754444e-09, -1.99834032e-17],\n",
       "       [-7.12027478e-01,  7.70943647e-01, -1.55821890e-01, ...,\n",
       "         3.11649234e-09,  2.08700501e-09,  3.92440934e-16],\n",
       "       ...,\n",
       "       [-5.07744749e-01, -1.29415329e-01, -8.22455123e-02, ...,\n",
       "        -1.62643372e-09,  3.38481898e-09,  7.38270018e-20],\n",
       "       [-4.79090569e-01, -8.63446712e-02, -1.58711407e-01, ...,\n",
       "        -4.39254174e-09,  1.67499503e-09, -1.47433165e-19],\n",
       "       [-4.42253816e-01, -1.06225249e-01, -4.71272464e-02, ...,\n",
       "        -3.03500819e-09,  3.98533616e-09,  3.30497068e-19]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit_transform(segData_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccf373db-0fb4-47e8-b50b-9e7f2c4e9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.07142340e-01 1.31969792e-01 1.01237729e-01 4.54353920e-02\n",
      " 3.54736114e-02 1.98803550e-02 1.89197030e-02 1.61540880e-02\n",
      " 1.06560085e-02 7.11337518e-03 3.92203525e-03 1.57500309e-03\n",
      " 4.89150725e-04 3.14164135e-05 2.36381041e-16 1.58615946e-16\n",
      " 1.52966845e-16 1.44466875e-16 9.39566232e-35]\n"
     ]
    }
   ],
   "source": [
    "# Get explained variance ratio\n",
    "varianceRatio = pca.explained_variance_ratio_\n",
    "print(varianceRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41900ec1-076a-4806-a9c4-176aee9052dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component: 1, Variance: 60.71423396853328\n",
      "Component: 2, Variance: 73.91121320168928\n",
      "Component: 3, Variance: 84.03498614256192\n",
      "Component: 4, Variance: 88.57852534332585\n",
      "Component: 5, Variance: 92.12588648109569\n",
      "Component: 6, Variance: 94.11392197960623\n",
      "Component: 7, Variance: 96.00589227704958\n",
      "Component: 8, Variance: 97.62130108194518\n",
      "Component: 9, Variance: 98.68690193362033\n",
      "Component: 10, Variance: 99.39823945149534\n",
      "Component: 11, Variance: 99.79044297671412\n",
      "Component: 12, Variance: 99.9479432861607\n",
      "Component: 13, Variance: 99.99685835864896\n",
      "Component: 14, Variance: 99.99999999999994\n",
      "Component: 15, Variance: 99.99999999999997\n",
      "Component: 16, Variance: 99.99999999999999\n",
      "Component: 17, Variance: 100.0\n",
      "Component: 18, Variance: 100.00000000000001\n",
      "Component: 19, Variance: 100.00000000000001\n"
     ]
    }
   ],
   "source": [
    "component = 0\n",
    "variance = 0\n",
    "for i in varianceRatio:\n",
    "    variance += i * 100\n",
    "    component += 1\n",
    "    print(f\"Component: {component}, Variance: {variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "974a1c93-29a7-4db8-a090-832971e009d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the scree plot\n",
    "plt.plot(range(1, len(varianceRatio) + 1), varianceRatio*100, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance Explained (%)')\n",
    "plt.title('Scree Plot')\n",
    "plt.xticks(range(1, len(varianceRatio) + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d0bd0-4c7a-46a9-ba51-81da154688c5",
   "metadata": {},
   "source": [
    "From scree plot and component breakdown above, seven components will explain 96% of the variance in the data. Therefore, we will go ahead reduce dimensions to 7 components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74c19dcb-466d-42aa-b5f3-57c9f7b153f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.68908218  0.53295103  0.24609833 ... -0.07643294  0.0478044\n",
      "  -0.04732089]\n",
      " [-0.66691952  0.51067465  0.33797172 ... -0.04117773  0.05655091\n",
      "  -0.04170747]\n",
      " [-0.71202748  0.77094365 -0.15582189 ... -0.16662228  0.04381397\n",
      "  -0.06069494]\n",
      " ...\n",
      " [-0.50774475 -0.12941533 -0.08224551 ... -0.02791769 -0.03189182\n",
      "  -0.10525024]\n",
      " [-0.47909057 -0.08634467 -0.15871141 ...  0.00443463 -0.00371285\n",
      "  -0.08505898]\n",
      " [-0.44225382 -0.10622525 -0.04712725 ...  0.01522668 -0.2061659\n",
      "   0.15294393]]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=7)\n",
    "reduced_segData_norm = pca.fit_transform(segData_norm)\n",
    "print(reduced_segData_norm)\n",
    "#Transform the data into a reduced dimension space using PCA with 7 components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28809d5-060d-41b5-b8d9-7a8150556dfc",
   "metadata": {},
   "source": [
    "#### d. Perform Kmeans again, but this time on the lower dimensional transformed data. Then, compute the Completeness and Homogeneity values of the new clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "885f5e3a-9435-4f61-8090-33242558c44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 7\n",
    "reduced_kMeans = KMeans(n_clusters=k)\n",
    "reduced_kMeans.fit(reduced_segData_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23c81b25-c1c0-46a0-b569-11d2929a5c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centroids:\n",
      "[-0.61926803  0.64024955  0.1958285  -0.08685588 -0.06775977  0.00886611\n",
      "  0.03837202]\n",
      "\n",
      "[ 0.43588833 -0.10529857  0.1652571   0.233499   -0.04503119 -0.00727238\n",
      "  0.01504941]\n",
      "\n",
      "[-0.60370483 -0.35550285  0.10919651 -0.12979947 -0.1309111  -0.02160283\n",
      " -0.04388193]\n",
      "\n",
      "[ 1.41452711  0.0872227   0.03676534 -0.17319537 -0.02992247 -0.00897281\n",
      " -0.02157291]\n",
      "\n",
      "[-0.20742333 -0.24636635  0.15253216  0.05661766  0.13031089 -0.00570357\n",
      "  0.03304526]\n",
      "\n",
      "[ 0.17640392  0.04369841 -0.26537004  0.18412442  0.02707514  0.02434118\n",
      "  0.00326357]\n",
      "\n",
      "[-0.51157863 -0.06490837 -0.33614472 -0.06536523  0.07880903  0.00623993\n",
      " -0.02626065]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Cluster Centroids:\")\n",
    "for reduced_centroid in reduced_kMeans.cluster_centers_:\n",
    "    print(reduced_centroid)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0483f238-78d6-4026-9ba9-707831d15546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centroids:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster 1</th>\n",
       "      <th>Cluster 2</th>\n",
       "      <th>Cluster 3</th>\n",
       "      <th>Cluster 4</th>\n",
       "      <th>Cluster 5</th>\n",
       "      <th>Cluster 6</th>\n",
       "      <th>Cluster 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PCA 1</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA 2</th>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA 3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA 4</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA 5</th>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA 6</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA 7</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cluster 1  Cluster 2  Cluster 3  Cluster 4  Cluster 5  Cluster 6  \\\n",
       "PCA 1      -0.62       0.44      -0.60       1.41      -0.21       0.18   \n",
       "PCA 2       0.64      -0.11      -0.36       0.09      -0.25       0.04   \n",
       "PCA 3       0.20       0.17       0.11       0.04       0.15      -0.27   \n",
       "PCA 4      -0.09       0.23      -0.13      -0.17       0.06       0.18   \n",
       "PCA 5      -0.07      -0.05      -0.13      -0.03       0.13       0.03   \n",
       "PCA 6       0.01      -0.01      -0.02      -0.01      -0.01       0.02   \n",
       "PCA 7       0.04       0.02      -0.04      -0.02       0.03       0.00   \n",
       "\n",
       "       Cluster 7  \n",
       "PCA 1      -0.51  \n",
       "PCA 2      -0.06  \n",
       "PCA 3      -0.34  \n",
       "PCA 4      -0.07  \n",
       "PCA 5       0.08  \n",
       "PCA 6       0.01  \n",
       "PCA 7      -0.03  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing cluster centroids\n",
    "print(\"Cluster Centroids:\")\n",
    "reduced_centroids = pd.DataFrame(reduced_kMeans.cluster_centers_, columns=[\"PCA {}\".format(i+1) for i in range(reduced_segData_norm.shape[1])])\n",
    "reduced_centroidsTranspose = reduced_centroids.T\n",
    "reduced_centroidsTranspose.columns = [\"Cluster {}\".format(i+1) for i in range(len(reduced_centroidsTranspose.columns))]\n",
    "reduced_centroidsTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a0c03c3-a5a7-41ce-a5c0-f61b2a11f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness with dimensionality reduction: 0.6110721173142056\n",
      "Homogeneity with dimensionality reduction: 0.6094646259197861\n"
     ]
    }
   ],
   "source": [
    "reduced_completeness = completeness_score(segClass['Value'], reduced_kMeans.labels_)\n",
    "reduced_homogeneity = homogeneity_score(segClass['Value'], reduced_kMeans.labels_)\n",
    "\n",
    "# Print the completeness and homogeneity scores\n",
    "print(f\"Completeness with dimensionality reduction: {reduced_completeness}\")\n",
    "print(f\"Homogeneity with dimensionality reduction: {reduced_homogeneity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b0f45-5f6c-4e67-937f-244cf5dfc594",
   "metadata": {},
   "source": [
    "#### e. Discuss your observations based on the comparison of the two clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bde9c986-03b3-459f-aa07-acbc5ae842ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness with full normalized dataset: 0.6117\n",
      "Homogeneity with full normalized dataset: 0.6100\n",
      "Completeness with dimensionality reduction on normalized dataset: 0.6111\n",
      "Homogeneity with dimensionality reduction on normalized dataset: 0.6095\n"
     ]
    }
   ],
   "source": [
    "print(f\"Completeness with full normalized dataset: {completeness:.4f}\")\n",
    "print(f\"Homogeneity with full normalized dataset: {homogeneity:.4f}\")\n",
    "print(f\"Completeness with dimensionality reduction on normalized dataset: {reduced_completeness:.4f}\")\n",
    "print(f\"Homogeneity with dimensionality reduction on normalized dataset: {reduced_homogeneity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04d8c2-25bb-4e91-b2c2-4642885ff71e",
   "metadata": {},
   "source": [
    "The Completeness and Homogeneity score results are very similar before and after dimensionality reduction. From this, for dataset with large number of features, it would be more efficient to run PCA first then clustering for confirmation. This will reduce the computational complexity and improving clustering process efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
